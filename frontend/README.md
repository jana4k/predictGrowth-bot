# GPT-Powered Startup Fundraising Q&A Bot

This project is a full-stack application that allows users to ask questions about startup fundraising. The answers are generated by an AI model (OpenRouter/Google AI) based *solely* on the content of a provided fundraising guide ("Raise Millions by Hustle Fund VC"). Users must be authenticated via Clerk to ask questions.

## Table of Contents

- [Project Overview](#project-overview)
- [Tech Stack](#tech-stack)
- [Features](#features)
- [Live Demo](#live-demo)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Local Development Setup](#local-development-setup)
  - [Backend Setup](#backend-setup)
  - [Frontend Setup](#frontend-setup)
- [Deployment](#deployment)
  - [Backend (Vercel)](#backend-vercel)
  - [Frontend (Vercel)](#frontend-vercel)
  - [Clerk Configuration](#clerk-configuration)
- [API Endpoints (Backend)](#api-endpoints-backend)
- [Approach Explanation](#approach-explanation)
- [Bonus Features Implemented](#bonus-features-implemented)
- [Potential Future Improvements](#potential-future-improvements)

## Project Overview

The application consists of:
1.  A **React.js frontend** using Ant Design components and Tailwind CSS for users to input questions and view AI-generated answers in a chat interface.
2.  A **Node.js (TypeScript) backend** API that processes questions, queries an LLM with a knowledge base, and returns structured answers.
3.  **Clerk authentication** to ensure only logged-in users can interact with the Q&A feature.
4.  **MongoDB Atlas** for storing Q&A history for authenticated users.

The core functionality revolves around providing accurate, document-grounded answers to fundraising-related queries.

## Tech Stack

**Frontend:**
*   React.js (with Vite)
*   TypeScript
*   Ant Design (Component Library)
*   Tailwind CSS (Styling)
*   Axios (HTTP Client)
*   Clerk (Authentication Client - `@clerk/clerk-react`)

**Backend:**
*   Node.js
*   Express.js (API Framework)
*   TypeScript
*   OpenRouter API & Google Generative AI API (LLM Services)
*   MongoDB Atlas (Database for Q&A history)
*   Mongoose (ODM for MongoDB)
*   Clerk (Authentication Middleware - `@clerk/clerk-sdk-node`)
*   Axios (HTTP Client for LLM APIs)
*   Dotenv (Environment Variables)
*   CORS

**Deployment:**
*   Vercel (for both Frontend and Backend as Serverless Functions)

## Features

*   User authentication via Clerk (Sign-up, Sign-in, Sign-out).
*   Authenticated users can ask questions about startup fundraising.
*   AI-generated answers based strictly on the provided `knowledge_base.txt` document.
*   Structured JSON responses from the LLM for different answer types (text, list).
*   Chat-style interface for Q&A.
*   Loading and error states in the UI.
*   Q&A history saved and viewable per user (MongoDB).
*   Responsive UI (thanks to Ant Design and Tailwind CSS).
*   (Attempted) Source section highlighting from the knowledge base in LLM responses.

## Live Demo

*   **Frontend:** [Link to your deployed Vercel Frontend URL]
*   **Backend API Base URL:** [Link to your deployed Vercel Backend URL, e.g., https://your-backend.vercel.app/api]

## Project Structure
predictgrowth/
├── backend/
│ ├── dist/ # Compiled backend output
│ ├── node_modules/
│ ├── src/
│ │ ├── authMiddleware.ts
│ │ ├── db.ts
│ │ ├── index.ts # Main Express app / Vercel entry
│ │ ├── knowledge_base.txt # Core document for Q&A
│ │ ├── llmService.ts # Logic for LLM interaction
│ │ └── models/
│ │ └── qaHistory.model.ts # Mongoose schema
│ ├── .env.example # Example environment variables
│ ├── package.json
│ ├── tsconfig.json
│ └── vercel.json # Vercel deployment configuration
├── frontend/
│ ├── dist/ # Vite build output
│ ├── node_modules/
│ ├── public/
│ ├── src/
│ │ ├── components/
│ │ │ └── ChatInterface.tsx
│ │ ├── App.tsx
│ │ ├── main.tsx
│ │ └── ... (other assets & css)
│ ├── .env.local.example # Example environment variables
│ ├── index.html
│ ├── package.json
│ ├── tailwind.config.js
│ ├── tsconfig.json
│ └── vite.config.ts
└── README.md

## Prerequisites

*   Node.js (v18.x or later recommended)
*   npm or yarn
*   Vercel Account (linked with GitHub/GitLab/Bitbucket)
*   Clerk Account
*   OpenAI/OpenRouter API Key
*   Google Generative AI API Key
*   MongoDB Atlas Account and a Free Tier Cluster

## Local Development Setup

### Backend Setup

1.  **Navigate to the backend directory:**
    ```bash
    cd backend
    ```
2.  **Install dependencies:**
    ```bash
    npm install
    ```
3.  **Create an environment file:**
    Copy `backend/.env.example` (if you create one) to `backend/.env` and fill in the following:
    ```env
    OPENROUTER_API_KEY=your_openrouter_api_key
    GOOGLE_AI_API_KEY=your_google_ai_api_key
    CLERK_SECRET_KEY=your_clerk_secret_key
    MONGODB_URI="your_mongodb_atlas_connection_string/yourDbName"
    # Optional for OpenRouter headers
    # YOUR_SITE_URL=http://localhost:5173 # Your local frontend URL
    # YOUR_APP_NAME=FundraisingQABot-Dev
    ```
4.  **Place `knowledge_base.txt`:**
    Ensure `backend/src/knowledge_base.txt` contains the content of the "Raise Millions by Hustle Fund VC" guide.
    *(If implementing source highlighting, also place `backend/src/structured_knowledge_base.json`)*
5.  **Run the development server:**
    ```bash
    npm run dev
    ```
    The backend will typically run on `http://localhost:3004`.

### Frontend Setup

1.  **Navigate to the frontend directory:**
    ```bash
    cd frontend
    ```
2.  **Install dependencies:**
    ```bash
    npm install
    ```
3.  **Create an environment file:**
    Copy `frontend/.env.local.example` (if you create one) to `frontend/.env.local` and fill in:
    ```env
    VITE_CLERK_PUBLISHABLE_KEY=your_clerk_publishable_key
    VITE_BACKEND_API_URL=http://localhost:3004/api # Or your backend's local dev port
    ```
4.  **Run the development server:**
    ```bash
    npm run dev
    ```
    The frontend will typically run on `http://localhost:5173`.

## Deployment

Both frontend and backend are deployed as separate projects on Vercel.

### Backend (Vercel)

1.  Ensure all code (including `backend/vercel.json`) is pushed to your Git repository.
2.  From the `backend/` directory, link the project to Vercel:
    ```bash
    vercel link
    ```
    Follow prompts to create a new project (e.g., `yourproject-backend`) or link to an existing one, ensuring the **Root Directory** is set to `.` (relative to the `backend` folder). Vercel should detect it as a Node.js project.
3.  Set **Environment Variables** in the Vercel project settings (Production, Preview, Development scopes):
    *   `OPENROUTER_API_KEY`
    *   `GOOGLE_AI_API_KEY`
    *   `CLERK_SECRET_KEY`
    *   `MONGODB_URI`
    *   `NODE_ENV=production`
    *   `YOUR_SITE_URL` (Your deployed frontend URL, for OpenRouter header)
    *   `YOUR_APP_NAME` (e.g., FundraisingQABot, for OpenRouter header)
4.  Deploy to production:
    ```bash
    vercel --prod
    ```
    Note the deployed backend URL (e.g., `https://your-backend.vercel.app`).

### Frontend (Vercel)

1.  Ensure all code is pushed to your Git repository.
2.  From the `frontend/` directory, link the project to Vercel:
    ```bash
    vercel link
    ```
    Follow prompts to create a new project (e.g., `yourproject-frontend`), ensuring the **Root Directory** is `.` (relative to the `frontend` folder). Vercel should detect it as a Vite project.
3.  Set **Environment Variables** in the Vercel project settings:
    *   `VITE_CLERK_PUBLISHABLE_KEY`
    *   `VITE_BACKEND_API_URL` (The full API URL of your deployed Vercel backend, e.g., `https://your-backend.vercel.app/api`)
4.  Deploy to production:
    ```bash
    vercel --prod
    ```
    Note the deployed frontend URL.

### Clerk Configuration

1.  Go to your Clerk Dashboard.
2.  Select your application.
3.  Navigate to **"Paths"** (or "URLs & Redirects").
    *   Update **Home URL, After sign-in URL, After sign-up URL, After sign-out URL** to your deployed Vercel **frontend URL**.
4.  Under **"API Keys"** or **"Domains"**, ensure your deployed Vercel frontend URL is listed as an allowed origin if necessary.

## API Endpoints (Backend)

All backend API routes are prefixed with `/api`.

*   **`GET /api/health`**: Health check endpoint.
    *   Response: `{ status: 'healthy', timestamp: '...' }`
*   **`POST /api/ask`** (Protected by Clerk Authentication):
    *   Request Body: `{ "question": "User's question string" }`
    *   Response (Success): `LLMStructuredResponse` (see `llmService.ts` for type definition - can be `text`, `list`).
    *   Response (Error): `{ "type": "error", "message": "Error message" }`
*   **`GET /api/history`** (Protected by Clerk Authentication):
    *   Response (Success): Array of `QAHistoryItem` objects (see `qaHistory.model.ts`).
    *   Response (Error): `{ "type": "error", "message": "Error message" }`

## Approach Explanation

*   **Backend:**
    *   An Express.js server handles API requests.
    *   Clerk middleware (`requireAuth`) protects the `/api/ask` and `/api/history` endpoints.
    *   The `llmService.ts` is responsible for:
        *   Loading the `knowledge_base.txt` file on startup.
        *   Constructing a detailed system prompt for the LLM, instructing it to answer based *only* on the provided document and to respond in a specific JSON format (`text` or `list` type).
        *   Making requests to OpenRouter (primary) or Google AI (fallback) with the user's question and the knowledge base content.
        *   Parsing and validating the LLM's JSON response.
    *   Mongoose is used to connect to MongoDB Atlas. The `/api/ask` route saves successful Q&A interactions, and `/api/history` retrieves them for the authenticated user.
    *   The application is deployed as a Vercel Serverless Function, with file assets like `knowledge_base.txt` included via `vercel.json` configuration.
*   **Frontend:**
    *   A Vite-powered React application.
    *   `ChatInterface.tsx` manages the chat UI, user input, and communication with the backend API.
    *   Clerk's `@clerk/clerk-react` handles user authentication flows (`<SignIn>`, `<UserButton>`, `useAuth` hook for token).
    *   Axios makes authenticated requests to the backend, including the Clerk JWT in the `Authorization` header.
    *   Ant Design provides UI components, and Tailwind CSS is used for layout and custom styling.
    *   A drawer component displays the user's Q&A history fetched from the backend.
*   **Prompt Engineering:** The system prompt for the LLM is carefully crafted to:
    *   Enforce grounding answers strictly to the provided document.
    *   Mandate JSON output.
    *   Define specific JSON structures for different answer types.
    *   Handle cases where the answer isn't in the document.
    *   (If implemented) Attempt to get source section information.

## Bonus Features Implemented

*   **Save Q&A history in MongoDB:** User interactions are saved and can be viewed in a history drawer.
*   **(Attempted/Partially Implemented) Highlight which section of the document the answer came from:** The LLM prompt and response structures were prepared for this. The success depends on LLM's adherence to providing `source_section_id` and `source_section_title`. The frontend is ready to display the `source_section_title` if provided by the backend.

## Potential Future Improvements

*   **Improved Source Highlighting:** More robust retrieval of relevant document sections before sending to LLM (e.g., using embeddings) for more accurate source attribution.
*   **Streaming Responses:** For a better UX with potentially slow LLM responses, stream the answer token by token. (Vercel serverless functions can support streaming).
*   **Error Reporting/Feedback:** Allow users to report incorrect answers.
*   **Pagination for History:** For users with extensive Q&A history.
*   **Advanced Search within History:**
*   **CI/CD with GitHub Actions:** Automate testing and deployment.
*   **More Sophisticated Document Chunking:** For very large knowledge bases to optimize LLM context.
*   **Unit and Integration Tests.**

---

Remember to replace bracketed placeholders like `[Link to your deployed Vercel Frontend URL]` with your actual information. This README provides a solid foundation for your project documentation.
